{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavishyapandit/VSCProjects/google-ai-hackathon24/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import re\n",
    "import json\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, temperature=0, safety_setting='BLOCK_NONE'):\n",
    "    \"\"\"\n",
    "    Generates a resopnse by hitting to Gemini\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): Description of the table.\n",
    "    - temperature: The DataFrame containing the file data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Data dictionary containing the description of the table, each column, and its data type.\n",
    "    \"\"\"\n",
    "    generation_config = {\n",
    "      \"temperature\": temperature,\n",
    "      \"top_p\": 1,\n",
    "      \"top_k\": 1,\n",
    "    }\n",
    "    safety_settings = [\n",
    "        {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": safety_setting\n",
    "        },\n",
    "        {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": safety_setting\n",
    "        },\n",
    "        {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": safety_setting\n",
    "        },\n",
    "        {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": safety_setting\n",
    "        },\n",
    "    ]\n",
    "    genai.configure(api_key=gemini_token['key'])\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
    "                                    generation_config=generation_config,\n",
    "                                    safety_settings=safety_settings)\n",
    "    convo = model.start_chat(history=[])\n",
    "    convo.send_message(prompt)\n",
    "    return re.sub(r\"\\*\\*([^*]+)\\*\\*\", r\"\\1\", convo.last.text)\n",
    "\n",
    "def auto_debugger(prompt, current_output, expected_output, temperature):\n",
    "    user_prompt = f'''\n",
    "        You are a helpful assisstant! Rewrite the below prompt so that the current output matches the expected output.\n",
    "        Prompt: {prompt}\n",
    "        Current Output: {current_output}\n",
    "        Expected Output: {expected_output}\n",
    "\n",
    "        Possible Solution(s):\n",
    "        1. Role: Assign a role to a LLM to make sure it behaves a certain way\n",
    "        2. Task: Clear explanation of the specific task to perform\n",
    "        3. Instructions: Certain instructions to abide by (in bullet points) to ensure the response generated is concise and relevant to the prompt\n",
    "        4. Output Format: An expected output format is shared (not the actual output)\n",
    "        5. Examples: Example that shows flow - from input to the LLM generated output where all the instructions are followed.\n",
    "\n",
    "        Adhere to below instructions at all costs!\n",
    "        Instruction:\n",
    "        1. Identify the cause of the error and rewrite the prompt - make it error free\n",
    "        2. Don't include any irrelevant text, header or footer in your response\n",
    "        3. Generate only refined prompt\n",
    "        4. Follow these instructions by all means\n",
    "        '''\n",
    "    debugged_code = generate_response(user_prompt, temperature=temperature)\n",
    "    return debugged_code\n",
    "\n",
    "def generate_good_prompt(prompt):\n",
    "    perfect_prompt = '''\n",
    "    In context to a Large Language Model (LLM) a prompt is an input or query that a user or a program gives to an LLM, in order to get a specific response from the model.\n",
    "    A good prompt comprises of the following things:\n",
    "\n",
    "    1. Role: Assign a role to a LLM to make sure it behaves a certain way\n",
    "    2. Task: Clear explanation of the specific task to perform\n",
    "    3. Instructions: Certain instructions to abide by (in bullet points) to ensure the response generated is concise and relevant to the prompt\n",
    "    4. Output Format: An expected output format is shared (not the actual output)\n",
    "    5. Examples: Example that shows flow - from input to the LLM generated output where all the instructions are followed.\n",
    "\n",
    "    Generate a perfect prompt:\n",
    "    '''\n",
    "\n",
    "    response = generate_response(perfect_prompt+prompt)\n",
    "    return response\n",
    "\n",
    "def generate_bad_prompt(prompt):\n",
    "    perfect_prompt = '''\n",
    "    In context to a Large Language Model (LLM) a prompt is an input or query that a user or a program gives to an LLM, in order to get a specific response from the model.\n",
    "    A bad prompt misses atleast one of the following things except for \"Task\" and can have spelling and grammatical mistakes:\n",
    "\n",
    "    1. Role: Assign a role to a LLM to make sure it behaves a certain way\n",
    "    2. Task: A specific task to perform\n",
    "    3. Instructions: Certain instructions to abide by (in bullet points) to ensure the response generated is concise and relevant to the prompt\n",
    "    4. Output Format: An expected output format is shared (not the actual output)\n",
    "    5. Examples: Example that shows flow - from input to the LLM generated output where all the instructions are followed.\n",
    "\n",
    "    Generate a bad prompt:\n",
    "    '''\n",
    "\n",
    "    response = generate_response(perfect_prompt+prompt)\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching credentials\n",
    "file = open('credentials.json', 'r')\n",
    "gemini_token = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following text in not more than 200 words:\n",
      "\n",
      "[Insert text here]\n"
     ]
    }
   ],
   "source": [
    "request = f'Write a prompt that summarises text from given text in not more than 200 words.'\n",
    "resp = generate_bad_prompt(request)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: Summarizer\n",
      "\n",
      "Task: Summarize the following text in not more than 200 words.\n",
      "\n",
      "Instructions:\n",
      "\n",
      "* Identify the key points and supporting details in the text.\n",
      "* Condense the information while maintaining the main ideas.\n",
      "* Use clear and concise language.\n",
      "* Avoid unnecessary details or repetitions.\n",
      "\n",
      "Output Format:\n",
      "\n",
      "A concise summary of the text, no longer than 200 words.\n",
      "\n",
      "Example:\n",
      "\n",
      "Input:\n",
      "\n",
      "The Large Language Model (LLM) is a powerful tool that can be used for a variety of tasks, including text summarization. LLMs are trained on massive datasets of text, which allows them to learn the patterns and structures of language. This knowledge enables them to generate summaries that are both accurate and informative.\n",
      "\n",
      "Output:\n",
      "\n",
      "LLMs are trained on vast text datasets, enabling them to understand language patterns and structures. This allows them to generate accurate and informative summaries by identifying key points and supporting details, condensing information, and using clear language.\n"
     ]
    }
   ],
   "source": [
    "request = f'Write a prompt that summarises text from given text in not more than 200 words.'\n",
    "resp = generate_good_prompt(request)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Name | Age | Profession | Nationality |\n",
      "|---|---|---|---|\n",
      "| John | 35 | Software Engineer | Canadian |\n"
     ]
    }
   ],
   "source": [
    "about_john = '''\n",
    "John age: 35, a Canadian national, is a software engineer with a meticulous personality.  He thrives on crafting clean, efficient code, ensuring every line serves a purpose. \n",
    "John's 10 years of experience building web applications have honed his skills in a vast programming language arsenal.  He isn't just proficient; he enjoys the intellectual \n",
    "challenge of mastering new languages and frameworks. This passion for innovation is a driving force in his career. John seeks opportunities to push boundaries and develop \n",
    "creative solutions to complex technical problems.  The satisfaction of seeing his code come to life as a functional, user-friendly application fuels his dedication. \n",
    "\n",
    "However, John's life isn't all ones and zeros.  Beyond the digital realm, he finds enjoyment in the physical world through competitive sports. The camaraderie and strategic \n",
    "elements of team sports like basketball provide a welcome contrast to the solitary focus of coding. When not on the court, John might be found in his garage, tinkering with \n",
    "electronics projects.  Taking apart old devices and repurposing components to create something new allows him to combine his technical expertise with a hands-on approach. \n",
    "This playful exploration keeps his mind sharp and fosters a sense of accomplishment outside the professional sphere.  John's life exemplifies a healthy balance between the \n",
    "meticulous problem-solving of a software engineer and the thrill of hands-on creation and physical activity.\n",
    "'''\n",
    "\n",
    "prompt = f'''Extract details like name, age, profession, nationality from the text.\n",
    "Text: {about_john}'''\n",
    "\n",
    "expected_output = '''{'Name':name, \n",
    "                'Age':age,\n",
    "                'Profession':profession,\n",
    "                'Nationality': nationality\n",
    "                }'''\n",
    "\n",
    "current_output = generate_response(prompt)\n",
    "print(current_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "    'Name': 'John',\n",
      "    'Age': 35,\n",
      "    'Profession': 'Software Engineer',\n",
      "    'Nationality': 'Canadian'\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "debugged_prompt = auto_debugger(prompt, current_output, expected_output, temperature=0.2)\n",
    "debugged_response = generate_response(debugged_prompt+f'Text:{prompt}')\n",
    "print(debugged_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
